# Web Scraper chatbot

## Overview
This project is a chatbot built using Streamlit, ChromaDB, and Groq's LLM to provide information about 01Crew. It scrapes website content using Selenium and BeautifulSoup, indexes the data into ChromaDB for retrieval, and utilizes a LangChain-based retrieval QA system with Groq's API for answering queries.

## Features
- **Web Scraping**: Uses Selenium to scrape website content and extract text.
- **Data Indexing**: Stores extracted data in ChromaDB as vector embeddings.
- **Retrieval Augmented Generation (RAG)**: Uses ChromaDB as a retriever to fetch relevant context for answering questions.
- **Groq API for Chatbot Responses**: Uses Groq's LLM to generate responses based on retrieved content.
- **Streamlit UI**: Provides a user-friendly interface for chatbot interaction.


### Required Python Packages
Install dependencies using:
```bash
pip install beautifulsoup4 chromadb langchain langchain_chroma langchain_groq selenium streamlit sentence-transformers tensorflow webdriver-manager
```

## How It Works
### Step 1: Scrape Website Content
The function `scrape_website(url)` uses Selenium to retrieve the full HTML of a webpage, extract its text using BeautifulSoup, and clean it into meaningful paragraphs.

### Step 2: Index Data in ChromaDB
The `index_data(chunks)` function initializes a persistent ChromaDB client and stores the scraped data as document embeddings.

### Step 3: Setup Retriever
The `setup_retriever(client, collection_name)` function uses a Sentence Transformer model to create vector embeddings for efficient text retrieval.

### Step 4: Create Retrieval QA Chain
The `setup_qa_chain(retriever)` function initializes Groq's ChatGroq LLM, which uses the retrieved context from ChromaDB to generate accurate responses.

### Step 5: Build the Chatbot Interface
The chatbot is implemented using Streamlit, where users can input queries and receive responses generated by Groq's LLM based on retrieved website content.

## Running the Project
1. Start the chatbot by running:
   ```bash
   streamlit run script.py
   ```
2. Enter the URL of a website in the sidebar and click **Scrape and Initialize**.
3. Once initialized, you can ask questions based on the scraped content.

## Future Improvements
- Support for multi-page website scraping.
- Option to upload documents for indexing.
- Integration with more LLM providers for improved responses.
